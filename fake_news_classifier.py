# -*- coding: utf-8 -*-
"""fake-news-classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//huggingface.co/geecee/Fake_News_Classifier/blob/main/fake-news-classifier.ipynb
"""

# 1. library import

# Commented out IPython magic to ensure Python compatibility.
from keras.models import Sequential
from keras.layers import Dense, LSTM, Embedding, Dropout
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

import numpy as np
import pandas as pd
import tensorflow as tf
import os

import nltk # NLP Exclusive package
from nltk.corpus import stopwords # Remove Stopwords
from nltk.stem import WordNetLemmatizer # Extraction
import re # Regular expression

# visualization
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
# %matplotlib inline

"""# 2. data load"""

# seed initialization
np.random.seed(0)
tf.random.set_seed(0)

Fake_df = pd.read_csv('../input/fake-and-real-news-dataset/Fake.csv')
True_df = pd.read_csv('../input/fake-and-real-news-dataset/True.csv')

Fake_df.shape, True_df.shape

# Fake_df.head().style.background_gradient(cmap='Greys', axis=1)
Fake_df.head()

True_df.head()

print(True_df.isna().sum(), Fake_df.isna().sum(), sep='\n\n')

Fake_df['target'] = 0
True_df['target'] = 1

# data merge
df = pd.concat([True_df, Fake_df], ignore_index=True, sort=False)
# ignore_index, sort
df.head()

"""# 3. graph"""

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning) # FutureWarning (Remove)

# Fake_df & True_df
plt.figure(figsize=(15, 5))
plt.subplot(1, 2, 1)
sns.countplot('subject', data=Fake_df)
plt.xticks(rotation =70)

plt.subplot(1, 2, 2)
sns.countplot('subject', data=True_df)
plt.show()

# Word_Cloud
from nltk.corpus import stopwords
stop_words = stopwords.words('english')

comment_words = ''
stopwords = set(stop_words)

# iterate through the csv file
for val in df[df['target']==1]['text']:

    # typecaste each val to string
    val = str(val)


    tokens = val.split()

    # Converts each token into lowercase
    for i in range(len(tokens)):
        tokens[i] = tokens[i].lower()

    comment_words += " ".join(tokens)+" "

wordcloud = WordCloud(width = 800, height = 800,
                background_color ='black',
                stopwords = stopwords,
                min_font_size = 10).generate(comment_words)

# plot the WordCloud image
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.show()

"""# 4. EDA

## 4.1. Data Cleaning
"""

df['text'] = df['title'] + ' ' + df['text'] # 'politicsNews About trump he was a president'

del df['title']
del df['subject']
del df['date']

df.head()

"""### 4.1.1.  Data cleaning example(first_text)
- remove functuations
- remove non alphabets
- convert Upper to Lower
- remove stopwords
- Lemmatizer
"""

first_text = df['text'][0]
first_text

first_text = re.sub('\[[^]]*\]', ' ', first_text) # remove punctuations
first_text = re.sub('[^a-zA-Z]',' ',first_text)  # replaces non-alphabets with spaces
first_text = first_text.lower() # Converting from uppercase to lowercase
first_text

# is, not, an (Remove Stopwords)
from nltk.corpus import stopwords
first_text = nltk.word_tokenize(first_text)
first_text = [word for word in first_text if not word in set(stopwords.words('english'))]

# lemmatization (Headword Extraction)
lemma = nltk.WordNetLemmatizer()
first_text = [lemma.lemmatize(word) for word in first_text]

first_text = ' '.join(first_text)
first_text

"""### 4.1.2. Data cleaning All
- different wordtokenize & split
- if str have more than 2 blink, wordtokenize can delete all blink. split can't
"""

# Remove punctuations
def remove_punctuations(text):
    return re.sub('\[[^]]*\]', '', text)

# Remove non-alphabets
def remove_characters(text):
    return re.sub('[^a-zA-Z]', ' ', text)

# Remove Stopwords
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
def remove_stopwords(text):
    return ' '.join([word for word in nltk.word_tokenize(text) if word not in stop_words])
# str(text).split()
# Headword Extraction
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
def lemmatize_words(text):
    return ' '.join(lemmatizer.lemmatize(word) for word in text.split())

def clean_text(text):
    text = text.lower()
    text = remove_punctuations(text)
    text = remove_characters(text)
    text = remove_stopwords(text)
    text = lemmatize_words(text)
    return text

# apply
df['text'] = df['text'].apply(clean_text)

df.head()

"""## 4.2. Preprocessing"""

# train_test split
X_train, X_test, y_train, y_test = train_test_split(df['text'], df['target'], test_size=0.3, random_state=0)

# tokenize
max_features = 10000 # 100 * 100
maxlen = 256 # avg maxlen 225

token = Tokenizer(num_words=max_features)
token.fit_on_texts(X_train)

# tokenize train
tokenized_train = token.texts_to_sequences(X_train)
x_train = pad_sequences(tokenized_train, maxlen=maxlen)

# tokenize test
tokenized_test = token.texts_to_sequences(X_test)
x_test = pad_sequences(tokenized_test, maxlen=maxlen)

"""# 5. Model

## 5.1. Modeling

### 5.1.1 LSTM
- This is the reason why use option at LSTM, didn't use dropout directly
- [link](https://wjddyd66.github.io/keras/Keras(4-2)/)
- trainable -> layer freeze
- many to many -> returns_sequences=True
"""

model = Sequential()
model.add(Embedding(max_features, output_dim=100, input_length=maxlen, trainable=False))
# layer freeze
model.add(LSTM(units=128, return_sequences=True, recurrent_dropout=0.25, dropout=0.25))
# return_sequences?
model.add(LSTM(units=64, recurrent_dropout=0.1, dropout=0.1))
model.add(Dropout(0.1))
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

from keras.callbacks import EarlyStopping, ModelCheckpoint

MODEL_DIR = './model/'
if not os.path.exists(MODEL_DIR):
    os.mkdir(MODEL_DIR)

modelpath = './model/{epoch:02d}-{val_loss:.4f}.hdf5'
Checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)

early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)
# model.fit(callbacks=[early_stopping_callback, Checkpointer])

history = model.fit(x_train, y_train, epochs=10, batch_size=256, validation_split=0.2, verbose=1, validation_data=(x_test, y_test),callbacks=[early_stopping_callback, Checkpointer])



print('Accuracy: {:.4f}'.format(model.evaluate(x_test, y_test)[1]))

"""## 5.2. model graph"""

y_vloss = history.history['val_loss']
y_loss = history.history['loss']
y_vacc = history.history['val_accuracy']
y_acc = history.history['accuracy']

x_len = np.arange(len(y_vloss))


plt.figure(figsize=(20, 5))

plt.subplot(1, 2, 1)
plt.title('Loss')
plt.plot(x_len, y_vloss, label='Testset')
plt.plot(x_len, y_loss, label='Trainset')
plt.grid()
plt.legend()
plt.xlabel('epochs')
plt.ylabel('loss')

plt.subplot(1, 2, 2)
plt.title('Accuracy')
plt.plot(x_len, y_vacc, label='Testset')
plt.plot(x_len, y_acc, label='Trainset')
plt.grid()
plt.legend()
plt.xlabel('epochs')
plt.ylabel('acc')

# epoch 5 is best model